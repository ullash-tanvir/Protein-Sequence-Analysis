{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821002da-53d1-4ae2-82e6-22a63b0ae872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c49587-3e24-4c39-9b1d-746b08bbbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffbc9fd9-af68-4572-a782-a94a40bc3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        encoded_seq = tokenizer.encode(\n",
    "            \" \".join(self.sequences[idx]), \n",
    "            add_special_tokens=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        return torch.tensor(encoded_seq), torch.tensor(self.labels[idx])\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_weights = self.attention(x)\n",
    "        attended_output = torch.sum(attention_weights * x, dim=1)\n",
    "        return attended_output, attention_weights\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, activation, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.activation = self._get_activation(activation)\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n",
    "        \n",
    "    def _get_activation(self, activation_name):\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(),\n",
    "            'gelu': nn.GELU(),\n",
    "            'selu': nn.SELU(),\n",
    "            'elu': nn.ELU()\n",
    "        }\n",
    "        return activations[activation_name]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.layers(x)\n",
    "        return self.activation(out + identity)\n",
    "\n",
    "class EnhancedProtBert(nn.Module):\n",
    "    def __init__(self, config, architecture_params):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # Freeze BERT base\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Architecture parameters\n",
    "        self.hidden_dims = architecture_params['hidden_dims']\n",
    "        self.dropout_rate = architecture_params['dropout_rate']\n",
    "        self.activation = architecture_params['activation']\n",
    "        self.use_attention = architecture_params['use_attention']\n",
    "        self.num_heads = architecture_params.get('num_heads', 1)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        if self.use_attention:\n",
    "            self.attention_heads = nn.ModuleList([\n",
    "                nn.MultiheadAttention(\n",
    "                    embed_dim=config.hidden_size,\n",
    "                    num_heads=self.num_heads,\n",
    "                    dropout=self.dropout_rate\n",
    "                ) for _ in range(self.num_heads)\n",
    "            ])\n",
    "            \n",
    "            # Attention pooling\n",
    "            self.attention_pool = AttentionLayer(config.hidden_size)\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        layers = []\n",
    "        input_dim = config.hidden_size * (self.num_heads if self.use_attention else 1)\n",
    "        \n",
    "        for hidden_dim in self.hidden_dims:\n",
    "            layers.extend([\n",
    "                ResidualBlock(\n",
    "                    input_dim,\n",
    "                    hidden_dim,\n",
    "                    self.activation,\n",
    "                    self.dropout_rate\n",
    "                )\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        self.feature_layers = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(self.hidden_dims[-1], config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get BERT outputs (frozen)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Apply attention if used\n",
    "        if self.use_attention:\n",
    "            attended_outputs = []\n",
    "            for attention in self.attention_heads:\n",
    "                attended, _ = attention(\n",
    "                    sequence_output.permute(1, 0, 2),\n",
    "                    sequence_output.permute(1, 0, 2),\n",
    "                    sequence_output.permute(1, 0, 2)\n",
    "                )\n",
    "                attended_outputs.append(attended.permute(1, 0, 2))\n",
    "            \n",
    "            # Concatenate attended outputs\n",
    "            sequence_output = torch.cat(attended_outputs, dim=-1)\n",
    "            \n",
    "            # Apply attention pooling\n",
    "            pooled_output, _ = self.attention_pool(sequence_output)\n",
    "        else:\n",
    "            # Global average pooling\n",
    "            pooled_output = sequence_output.mean(dim=1)\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = self.feature_layers(pooled_output)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "class DynamicLearningRate:\n",
    "    def __init__(self, optimizer, mode='cosine', **kwargs):\n",
    "        self.optimizer = optimizer\n",
    "        self.mode = mode\n",
    "        self.kwargs = kwargs\n",
    "        self.schedulers = {\n",
    "            'cosine': self._get_cosine_scheduler,\n",
    "            'one_cycle': self._get_one_cycle_scheduler,\n",
    "            'cyclic': self._get_cyclic_scheduler\n",
    "        }\n",
    "        self.scheduler = self.schedulers[mode]()\n",
    "\n",
    "    def _get_cosine_scheduler(self):\n",
    "        return get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.kwargs.get('num_warmup_steps', 100),\n",
    "            num_training_steps=self.kwargs.get('num_training_steps', 1000)\n",
    "        )\n",
    "\n",
    "    def _get_one_cycle_scheduler(self):\n",
    "        return torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.kwargs.get('max_lr', 0.01),\n",
    "            epochs=self.kwargs.get('epochs', 10),\n",
    "            steps_per_epoch=self.kwargs.get('steps_per_epoch', 100)\n",
    "        )\n",
    "\n",
    "    def _get_cyclic_scheduler(self):\n",
    "        return torch.optim.lr_scheduler.CyclicLR(\n",
    "            self.optimizer,\n",
    "            base_lr=self.kwargs.get('base_lr', 1e-4),\n",
    "            max_lr=self.kwargs.get('max_lr', 1e-3),\n",
    "            cycle_momentum=False\n",
    "        )\n",
    "\n",
    "    def step(self):\n",
    "        self.scheduler.step()\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, average='binary'),\n",
    "        'recall': recall_score(all_labels, all_preds, average='binary'),\n",
    "        'f1': f1_score(all_labels, all_preds, average='binary'),\n",
    "        'auc_roc': roc_auc_score(all_labels, all_probs)\n",
    "    }\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    metrics['roc_curve'] = {\n",
    "        'fpr': fpr.tolist(),\n",
    "        'tpr': tpr.tolist()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, lr_scheduler, device, dataset_name, fold=None, num_epochs=50):\n",
    "    \"\"\"Train model and track metrics for each epoch.\"\"\"\n",
    "    best_val_acc = 0\n",
    "    best_model = None\n",
    "    training_history = []\n",
    "    \n",
    "    # Create directory for plots\n",
    "    plots_dir = os.path.join('results', dataset_name, 'plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(inputs, labels=labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_metrics = {\n",
    "            'loss': train_loss / len(train_loader),\n",
    "            'accuracy': accuracy_score(train_labels, train_preds),\n",
    "            'precision': precision_score(train_labels, train_preds, average='binary'),\n",
    "            'recall': recall_score(train_labels, train_preds, average='binary'),\n",
    "            'f1': f1_score(train_labels, train_preds, average='binary')\n",
    "        }\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Save epoch results\n",
    "        epoch_results = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_metrics': train_metrics,\n",
    "            'val_metrics': val_metrics,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        training_history.append(epoch_results)\n",
    "        \n",
    "        # Update best model\n",
    "        if val_metrics['accuracy'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['accuracy']\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "            # Save ROC curve for best model\n",
    "            fold_suffix = f'_fold_{fold}' if fold is not None else ''\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(val_metrics['roc_curve']['fpr'], \n",
    "                    val_metrics['roc_curve']['tpr'], \n",
    "                    label=f'ROC curve (AUC = {val_metrics[\"auc_roc\"]:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {dataset_name}{fold_suffix}')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(os.path.join(plots_dir, f'roc_curve{fold_suffix}.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save training history plot\n",
    "    fold_suffix = f'_fold_{fold}' if fold is not None else ''\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.plot(epochs, [h['train_metrics']['accuracy'] for h in training_history], label='Train Acc')\n",
    "    plt.plot(epochs, [h['val_metrics']['accuracy'] for h in training_history], label='Val Acc')\n",
    "    plt.title(f'Training History - {dataset_name}{fold_suffix}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, f'training_history{fold_suffix}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return best_model, training_history\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(seq) for seq in sequences], \n",
    "        batch_first=True, \n",
    "        padding_value=0\n",
    "    )\n",
    "    labels = torch.tensor(labels)\n",
    "    return padded_sequences, labels\n",
    "\n",
    "def get_grid_search_params():\n",
    "    \"\"\"Define parameter grid for search.\"\"\"\n",
    "    return {\n",
    "        'architecture': {\n",
    "            'hidden_dims': [[512, 256], [256, 128], [512, 256, 128]],\n",
    "            'dropout_rate': [0.1, 0.2, 0.3],\n",
    "            'activation': ['relu', 'gelu'],\n",
    "            'use_attention': [True, False],\n",
    "            'num_heads': [1, 2, 4]\n",
    "        },\n",
    "        'training': {\n",
    "            'batch_size': [16, 32],\n",
    "            'base_lr': [1e-4, 2e-4],\n",
    "            'max_lr': [1e-3, 2e-3],\n",
    "            'weight_decay': [0.01, 0.1],\n",
    "            'warmup_steps': [100, 200],\n",
    "            'lr_scheduler': ['cosine', 'one_cycle']\n",
    "        }\n",
    "    }\n",
    "\n",
    "def run_kfold_grid_search(train_dataset, test_dataset, dataset_name, device, n_splits=5):\n",
    "    \"\"\"Run grid search with k-fold CV and save comprehensive metrics.\"\"\"\n",
    "    # Create results directory\n",
    "    results_dir = os.path.join('results', dataset_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    params_grid = get_grid_search_params()\n",
    "    best_result = {\n",
    "        'dataset': dataset_name,\n",
    "        'cv_metrics': {\n",
    "            'accuracy': 0,\n",
    "            'precision': 0,\n",
    "            'recall': 0,\n",
    "            'f1': 0,\n",
    "            'auc_roc': 0\n",
    "        },\n",
    "        'params': None,\n",
    "        'test_metrics': None,\n",
    "        'fold_histories': [],\n",
    "        'fold_metrics': []\n",
    "    }\n",
    "    \n",
    "    # Create test loader once\n",
    "    # Add drop_last=False here because we don't strictly need it for testing/evaluation\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        drop_last=False  # Typically okay for evaluation, doesn't affect BatchNorm in eval mode\n",
    "    )\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    arch_params = [dict(zip(params_grid['architecture'].keys(), v)) \n",
    "                  for v in product(*params_grid['architecture'].values())]\n",
    "    train_params = [dict(zip(params_grid['training'].keys(), v)) \n",
    "                   for v in product(*params_grid['training'].values())]\n",
    "    \n",
    "    total_combinations = len(arch_params) * len(train_params)\n",
    "    \n",
    "    # Initialize K-fold\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for comb_idx, (arch, train) in enumerate(product(arch_params, train_params)):\n",
    "        print(f\"\\nParameter combination {comb_idx + 1}/{total_combinations}\")\n",
    "        params = {'architecture': arch, 'training': train}\n",
    "        fold_histories = []\n",
    "        fold_metrics = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "            print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "            \n",
    "            # Create data loaders for this fold\n",
    "            # IMPORTANT: drop_last=True to avoid single-sample batches\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=train['batch_size'],\n",
    "                sampler=SubsetRandomSampler(train_idx),\n",
    "                collate_fn=custom_collate_fn,\n",
    "                drop_last=True  # <--- CHANGE HERE\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=train['batch_size'],\n",
    "                sampler=SubsetRandomSampler(val_idx),\n",
    "                collate_fn=custom_collate_fn,\n",
    "                drop_last=True  # <--- CHANGE HERE\n",
    "            )\n",
    "            \n",
    "            # Initialize model\n",
    "            config = BertConfig.from_pretrained(\n",
    "                \"Rostlab/prot_bert\",\n",
    "                num_labels=2\n",
    "            )\n",
    "            model = EnhancedProtBert(config, arch).to(device)\n",
    "            \n",
    "            # Initialize optimizer and scheduler\n",
    "            optimizer = AdamW(\n",
    "                model.parameters(),\n",
    "                lr=train['base_lr'],\n",
    "                weight_decay=train['weight_decay']\n",
    "            )\n",
    "            \n",
    "            lr_scheduler = DynamicLearningRate(\n",
    "                optimizer,\n",
    "                mode=train['lr_scheduler'],\n",
    "                num_warmup_steps=train['warmup_steps'],\n",
    "                max_lr=train['max_lr'],\n",
    "                num_training_steps=50 * len(train_loader)\n",
    "            )\n",
    "            \n",
    "            # Train model for this fold\n",
    "            model, history = train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                optimizer=optimizer,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                device=device,\n",
    "                dataset_name=dataset_name,\n",
    "                fold=fold + 1\n",
    "            )\n",
    "            \n",
    "            # Save fold results\n",
    "            fold_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'metrics': evaluate(model, val_loader, device)\n",
    "            })\n",
    "            fold_histories.append({\n",
    "                'fold': fold + 1,\n",
    "                'history': history\n",
    "            })\n",
    "            \n",
    "            # Clean up\n",
    "            del model, optimizer, lr_scheduler\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calculate average CV metrics\n",
    "        avg_cv_metrics = {\n",
    "            'accuracy': np.mean([m['metrics']['accuracy'] for m in fold_metrics]),\n",
    "            'precision': np.mean([m['metrics']['precision'] for m in fold_metrics]),\n",
    "            'recall': np.mean([m['metrics']['recall'] for m in fold_metrics]),\n",
    "            'f1': np.mean([m['metrics']['f1'] for m in fold_metrics]),\n",
    "            'auc_roc': np.mean([m['metrics']['auc_roc'] for m in fold_metrics])\n",
    "        }\n",
    "        \n",
    "        print(f\"Average CV metrics:\")\n",
    "        for metric, value in avg_cv_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Update best result if current is better\n",
    "        if avg_cv_metrics['accuracy'] > best_result['cv_metrics']['accuracy']:\n",
    "            print(\"New best parameters found!\")\n",
    "            \n",
    "            # Train final model with best parameters on full training set\n",
    "            config = BertConfig.from_pretrained(\n",
    "                \"Rostlab/prot_bert\",\n",
    "                num_labels=2\n",
    "            )\n",
    "            final_model = EnhancedProtBert(config, arch).to(device)\n",
    "            \n",
    "            # IMPORTANT: Also drop_last=True here if you want to avoid any single-batch in final training\n",
    "            final_train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=train['batch_size'],\n",
    "                shuffle=True,\n",
    "                collate_fn=custom_collate_fn,\n",
    "                drop_last=True  # <--- CHANGE HERE\n",
    "            )\n",
    "            \n",
    "            optimizer = AdamW(\n",
    "                final_model.parameters(),\n",
    "                lr=train['base_lr'],\n",
    "                weight_decay=train['weight_decay']\n",
    "            )\n",
    "            \n",
    "            lr_scheduler = DynamicLearningRate(\n",
    "                optimizer,\n",
    "                mode=train['lr_scheduler'],\n",
    "                num_warmup_steps=train['warmup_steps'],\n",
    "                max_lr=train['max_lr'],\n",
    "                num_training_steps=50 * len(final_train_loader)\n",
    "            )\n",
    "            \n",
    "            # Train on full training set\n",
    "            final_model, _ = train_model(\n",
    "                model=final_model,\n",
    "                train_loader=final_train_loader,\n",
    "                val_loader=test_loader,\n",
    "                optimizer=optimizer,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                device=device,\n",
    "                dataset_name=dataset_name\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_metrics = evaluate(final_model, test_loader, device)\n",
    "            \n",
    "            best_result = {\n",
    "                'dataset': dataset_name,\n",
    "                'cv_metrics': avg_cv_metrics,\n",
    "                'params': params,\n",
    "                'test_metrics': test_metrics,\n",
    "                'fold_histories': fold_histories,\n",
    "                'fold_metrics': fold_metrics,\n",
    "                'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            }\n",
    "            \n",
    "            # Save best parameters and results\n",
    "            results_file = os.path.join(results_dir, 'best_results.json')\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(best_result, f, indent=4)\n",
    "            \n",
    "            del final_model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055e14de-f436-4d86-a5e3-85f0e255e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Processing dataset: APV\n",
      "\n",
      "Parameter combination 1/6912\n",
      "Fold 1/5\n",
      "Epoch 1/50\n",
      "Train Loss: 0.6651, Acc: 0.5781\n",
      "Val Acc: 0.6429, F1: 0.1304\n",
      "Epoch 2/50\n",
      "Train Loss: 0.4756, Acc: 0.7879\n",
      "Val Acc: 0.8304, F1: 0.7865\n",
      "Epoch 3/50\n",
      "Train Loss: 0.4011, Acc: 0.8170\n",
      "Val Acc: 0.6429, F1: 0.0476\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3478, Acc: 0.8661\n",
      "Val Acc: 0.7589, F1: 0.7327\n",
      "Epoch 5/50\n",
      "Train Loss: 0.3380, Acc: 0.8504\n",
      "Val Acc: 0.3571, F1: 0.5263\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2724, Acc: 0.8951\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 7/50\n",
      "Train Loss: 0.2661, Acc: 0.8795\n",
      "Val Acc: 0.6696, F1: 0.6838\n",
      "Epoch 8/50\n",
      "Train Loss: 0.2412, Acc: 0.9062\n",
      "Val Acc: 0.8929, F1: 0.8636\n",
      "Epoch 9/50\n",
      "Train Loss: 0.1932, Acc: 0.9196\n",
      "Val Acc: 0.8571, F1: 0.8333\n",
      "Epoch 10/50\n",
      "Train Loss: 0.2146, Acc: 0.9241\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 11/50\n",
      "Train Loss: 0.1656, Acc: 0.9353\n",
      "Val Acc: 0.8304, F1: 0.7077\n",
      "Epoch 12/50\n",
      "Train Loss: 0.1961, Acc: 0.9263\n",
      "Val Acc: 0.3661, F1: 0.5298\n",
      "Epoch 13/50\n",
      "Train Loss: 0.1797, Acc: 0.9330\n",
      "Val Acc: 0.7589, F1: 0.5091\n",
      "Epoch 14/50\n",
      "Train Loss: 0.1236, Acc: 0.9665\n",
      "Val Acc: 0.7321, F1: 0.7273\n",
      "Epoch 15/50\n",
      "Train Loss: 0.1210, Acc: 0.9509\n",
      "Val Acc: 0.4107, F1: 0.5541\n",
      "Epoch 16/50\n",
      "Train Loss: 0.1419, Acc: 0.9464\n",
      "Val Acc: 0.3661, F1: 0.5359\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1511, Acc: 0.9509\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 18/50\n",
      "Train Loss: 0.1175, Acc: 0.9464\n",
      "Val Acc: 0.6696, F1: 0.1778\n",
      "Epoch 19/50\n",
      "Train Loss: 0.0974, Acc: 0.9688\n",
      "Val Acc: 0.4821, F1: 0.5735\n",
      "Epoch 20/50\n",
      "Train Loss: 0.1075, Acc: 0.9598\n",
      "Val Acc: 0.7321, F1: 0.7059\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0876, Acc: 0.9688\n",
      "Val Acc: 0.8482, F1: 0.7536\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0816, Acc: 0.9732\n",
      "Val Acc: 0.5893, F1: 0.6034\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0958, Acc: 0.9621\n",
      "Val Acc: 0.8125, F1: 0.7586\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0730, Acc: 0.9821\n",
      "Val Acc: 0.5357, F1: 0.6119\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0663, Acc: 0.9754\n",
      "Val Acc: 0.8482, F1: 0.7792\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0553, Acc: 0.9821\n",
      "Val Acc: 0.8393, F1: 0.7568\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0437, Acc: 0.9866\n",
      "Val Acc: 0.8393, F1: 0.7750\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0397, Acc: 0.9888\n",
      "Val Acc: 0.8571, F1: 0.8049\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0853, Acc: 0.9732\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0482, Acc: 0.9844\n",
      "Val Acc: 0.7054, F1: 0.6796\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0362, Acc: 0.9911\n",
      "Val Acc: 0.8214, F1: 0.7727\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0515, Acc: 0.9866\n",
      "Val Acc: 0.7500, F1: 0.6957\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0403, Acc: 0.9866\n",
      "Val Acc: 0.8036, F1: 0.7755\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0413, Acc: 0.9888\n",
      "Val Acc: 0.5804, F1: 0.6357\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0467, Acc: 0.9866\n",
      "Val Acc: 0.8393, F1: 0.7500\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0297, Acc: 0.9911\n",
      "Val Acc: 0.8214, F1: 0.7826\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0219, Acc: 0.9978\n",
      "Val Acc: 0.8125, F1: 0.7342\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0190, Acc: 0.9955\n",
      "Val Acc: 0.7946, F1: 0.6462\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0265, Acc: 0.9911\n",
      "Val Acc: 0.8571, F1: 0.7895\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0193, Acc: 0.9978\n",
      "Val Acc: 0.8304, F1: 0.7711\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0192, Acc: 0.9978\n",
      "Val Acc: 0.8571, F1: 0.7949\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0334, Acc: 0.9911\n",
      "Val Acc: 0.8393, F1: 0.7500\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0428, Acc: 0.9888\n",
      "Val Acc: 0.8304, F1: 0.7324\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0284, Acc: 0.9911\n",
      "Val Acc: 0.8393, F1: 0.7805\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0284, Acc: 0.9911\n",
      "Val Acc: 0.8214, F1: 0.7619\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0132, Acc: 1.0000\n",
      "Val Acc: 0.8393, F1: 0.7568\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0331, Acc: 0.9933\n",
      "Val Acc: 0.8661, F1: 0.8052\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0236, Acc: 0.9978\n",
      "Val Acc: 0.8036, F1: 0.7179\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0179, Acc: 0.9978\n",
      "Val Acc: 0.8482, F1: 0.7848\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0201, Acc: 0.9978\n",
      "Val Acc: 0.8393, F1: 0.7805\n",
      "Fold 2/5\n",
      "Epoch 1/50\n",
      "Train Loss: 0.6364, Acc: 0.6004\n",
      "Val Acc: 0.6071, F1: 0.0000\n",
      "Epoch 2/50\n",
      "Train Loss: 0.4513, Acc: 0.7835\n",
      "Val Acc: 0.6071, F1: 0.0000\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3841, Acc: 0.8393\n",
      "Val Acc: 0.6161, F1: 0.0851\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3540, Acc: 0.8281\n",
      "Val Acc: 0.7679, F1: 0.6061\n",
      "Epoch 5/50\n",
      "Train Loss: 0.3027, Acc: 0.8728\n",
      "Val Acc: 0.5536, F1: 0.6377\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2574, Acc: 0.8906\n",
      "Val Acc: 0.4732, F1: 0.5986\n",
      "Epoch 7/50\n",
      "Train Loss: 0.2351, Acc: 0.9040\n",
      "Val Acc: 0.6161, F1: 0.0851\n",
      "Epoch 8/50\n",
      "Train Loss: 0.2308, Acc: 0.9018\n",
      "Val Acc: 0.4732, F1: 0.5986\n",
      "Epoch 9/50\n",
      "Train Loss: 0.2170, Acc: 0.9040\n",
      "Val Acc: 0.6250, F1: 0.1250\n",
      "Epoch 10/50\n",
      "Train Loss: 0.1980, Acc: 0.9286\n",
      "Val Acc: 0.7500, F1: 0.5758\n",
      "Epoch 11/50\n",
      "Train Loss: 0.1475, Acc: 0.9531\n",
      "Val Acc: 0.4196, F1: 0.5752\n",
      "Epoch 12/50\n",
      "Train Loss: 0.1348, Acc: 0.9442\n",
      "Val Acc: 0.6071, F1: 0.0000\n",
      "Epoch 13/50\n",
      "Train Loss: 0.2008, Acc: 0.9107\n",
      "Val Acc: 0.5625, F1: 0.5421\n",
      "Epoch 14/50\n",
      "Train Loss: 0.1452, Acc: 0.9464\n",
      "Val Acc: 0.3929, F1: 0.5641\n",
      "Epoch 15/50\n",
      "Train Loss: 0.1638, Acc: 0.9397\n",
      "Val Acc: 0.6071, F1: 0.0000\n",
      "Epoch 16/50\n",
      "Train Loss: 0.1275, Acc: 0.9464\n",
      "Val Acc: 0.4375, F1: 0.5828\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1087, Acc: 0.9598\n",
      "Val Acc: 0.3929, F1: 0.5641\n",
      "Epoch 18/50\n",
      "Train Loss: 0.1550, Acc: 0.9420\n",
      "Val Acc: 0.7946, F1: 0.7473\n",
      "Epoch 19/50\n",
      "Train Loss: 0.1004, Acc: 0.9643\n",
      "Val Acc: 0.7679, F1: 0.7292\n",
      "Epoch 20/50\n",
      "Train Loss: 0.1146, Acc: 0.9665\n",
      "Val Acc: 0.6071, F1: 0.0000\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0950, Acc: 0.9710\n",
      "Val Acc: 0.6339, F1: 0.1633\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0668, Acc: 0.9777\n",
      "Val Acc: 0.4196, F1: 0.5752\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0718, Acc: 0.9732\n",
      "Val Acc: 0.7232, F1: 0.7103\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0632, Acc: 0.9799\n",
      "Val Acc: 0.7143, F1: 0.7091\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0491, Acc: 0.9866\n",
      "Val Acc: 0.8125, F1: 0.7407\n",
      "Epoch 26/50\n",
      "Train Loss: 0.1048, Acc: 0.9665\n",
      "Val Acc: 0.7054, F1: 0.7027\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0613, Acc: 0.9821\n",
      "Val Acc: 0.8304, F1: 0.7765\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0559, Acc: 0.9821\n",
      "Val Acc: 0.6161, F1: 0.0851\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0405, Acc: 0.9933\n",
      "Val Acc: 0.6696, F1: 0.3019\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0202, Acc: 1.0000\n",
      "Val Acc: 0.7679, F1: 0.7234\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0349, Acc: 0.9888\n",
      "Val Acc: 0.6071, F1: 0.6615\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0435, Acc: 0.9866\n",
      "Val Acc: 0.8304, F1: 0.7765\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0605, Acc: 0.9777\n",
      "Val Acc: 0.8125, F1: 0.7342\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0305, Acc: 0.9955\n",
      "Val Acc: 0.6875, F1: 0.6903\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0359, Acc: 0.9866\n",
      "Val Acc: 0.7321, F1: 0.7059\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0428, Acc: 0.9888\n",
      "Val Acc: 0.8214, F1: 0.7917\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0285, Acc: 0.9933\n",
      "Val Acc: 0.4554, F1: 0.5850\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0372, Acc: 0.9866\n",
      "Val Acc: 0.7946, F1: 0.7579\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0251, Acc: 0.9955\n",
      "Val Acc: 0.7768, F1: 0.7368\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0186, Acc: 0.9911\n",
      "Val Acc: 0.8036, F1: 0.7556\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0206, Acc: 0.9955\n",
      "Val Acc: 0.7946, F1: 0.7579\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0230, Acc: 0.9933\n",
      "Val Acc: 0.7500, F1: 0.7255\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0342, Acc: 0.9933\n",
      "Val Acc: 0.7946, F1: 0.7473\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0370, Acc: 0.9844\n",
      "Val Acc: 0.8125, F1: 0.7470\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0301, Acc: 0.9888\n",
      "Val Acc: 0.8036, F1: 0.7660\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0221, Acc: 0.9933\n",
      "Val Acc: 0.7768, F1: 0.7525\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0165, Acc: 0.9978\n",
      "Val Acc: 0.7500, F1: 0.7200\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0262, Acc: 0.9911\n",
      "Val Acc: 0.8036, F1: 0.7755\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0316, Acc: 0.9911\n",
      "Val Acc: 0.7679, F1: 0.7451\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0225, Acc: 0.9955\n",
      "Val Acc: 0.7768, F1: 0.7475\n",
      "Fold 3/5\n",
      "Epoch 1/50\n",
      "Train Loss: 0.6782, Acc: 0.6027\n",
      "Val Acc: 0.5357, F1: 0.0000\n",
      "Epoch 2/50\n",
      "Train Loss: 0.4695, Acc: 0.7634\n",
      "Val Acc: 0.5357, F1: 0.0000\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3450, Acc: 0.8616\n",
      "Val Acc: 0.5446, F1: 0.0377\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3518, Acc: 0.8415\n",
      "Val Acc: 0.5625, F1: 0.1091\n",
      "Epoch 5/50\n",
      "Train Loss: 0.2918, Acc: 0.8884\n",
      "Val Acc: 0.6429, F1: 0.3939\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2567, Acc: 0.8906\n",
      "Val Acc: 0.4732, F1: 0.6335\n",
      "Epoch 7/50\n",
      "Train Loss: 0.2117, Acc: 0.9107\n",
      "Val Acc: 0.6964, F1: 0.7500\n",
      "Epoch 8/50\n",
      "Train Loss: 0.1767, Acc: 0.9330\n",
      "Val Acc: 0.4643, F1: 0.6341\n",
      "Epoch 9/50\n",
      "Train Loss: 0.2222, Acc: 0.8996\n",
      "Val Acc: 0.5357, F1: 0.0000\n",
      "Epoch 10/50\n",
      "Train Loss: 0.1528, Acc: 0.9420\n",
      "Val Acc: 0.6161, F1: 0.3385\n",
      "Epoch 11/50\n",
      "Train Loss: 0.1380, Acc: 0.9464\n",
      "Val Acc: 0.4643, F1: 0.6341\n",
      "Epoch 12/50\n",
      "Train Loss: 0.1524, Acc: 0.9420\n",
      "Val Acc: 0.5268, F1: 0.0000\n",
      "Epoch 13/50\n",
      "Train Loss: 0.1384, Acc: 0.9397\n",
      "Val Acc: 0.4643, F1: 0.6341\n",
      "Epoch 14/50\n",
      "Train Loss: 0.1417, Acc: 0.9531\n",
      "Val Acc: 0.6696, F1: 0.5067\n",
      "Epoch 15/50\n",
      "Train Loss: 0.1118, Acc: 0.9598\n",
      "Val Acc: 0.7143, F1: 0.7576\n",
      "Epoch 16/50\n",
      "Train Loss: 0.0956, Acc: 0.9554\n",
      "Val Acc: 0.7500, F1: 0.7021\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1094, Acc: 0.9576\n",
      "Val Acc: 0.5893, F1: 0.2581\n",
      "Epoch 18/50\n",
      "Train Loss: 0.0869, Acc: 0.9665\n",
      "Val Acc: 0.7411, F1: 0.7717\n",
      "Epoch 19/50\n",
      "Train Loss: 0.0923, Acc: 0.9688\n",
      "Val Acc: 0.4732, F1: 0.6380\n",
      "Epoch 20/50\n",
      "Train Loss: 0.0878, Acc: 0.9710\n",
      "Val Acc: 0.7054, F1: 0.5926\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0647, Acc: 0.9777\n",
      "Val Acc: 0.7679, F1: 0.7451\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0779, Acc: 0.9732\n",
      "Val Acc: 0.6161, F1: 0.3582\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0654, Acc: 0.9732\n",
      "Val Acc: 0.7500, F1: 0.7544\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0646, Acc: 0.9754\n",
      "Val Acc: 0.5893, F1: 0.2333\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0547, Acc: 0.9777\n",
      "Val Acc: 0.7500, F1: 0.7544\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0509, Acc: 0.9911\n",
      "Val Acc: 0.4821, F1: 0.6420\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0446, Acc: 0.9911\n",
      "Val Acc: 0.6250, F1: 0.3824\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0484, Acc: 0.9844\n",
      "Val Acc: 0.7143, F1: 0.7460\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0507, Acc: 0.9844\n",
      "Val Acc: 0.5089, F1: 0.6541\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0366, Acc: 0.9911\n",
      "Val Acc: 0.5357, F1: 0.0000\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0626, Acc: 0.9799\n",
      "Val Acc: 0.7143, F1: 0.6098\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0307, Acc: 0.9911\n",
      "Val Acc: 0.7232, F1: 0.6517\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0349, Acc: 0.9911\n",
      "Val Acc: 0.7054, F1: 0.7442\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0201, Acc: 0.9978\n",
      "Val Acc: 0.7768, F1: 0.7368\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0365, Acc: 0.9866\n",
      "Val Acc: 0.5982, F1: 0.3077\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0314, Acc: 0.9911\n",
      "Val Acc: 0.7768, F1: 0.7423\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0322, Acc: 0.9911\n",
      "Val Acc: 0.6964, F1: 0.5750\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0245, Acc: 0.9978\n",
      "Val Acc: 0.7946, F1: 0.7579\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0204, Acc: 0.9978\n",
      "Val Acc: 0.8125, F1: 0.7879\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0134, Acc: 0.9978\n",
      "Val Acc: 0.7411, F1: 0.7603\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0184, Acc: 0.9978\n",
      "Val Acc: 0.7857, F1: 0.7895\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0139, Acc: 0.9978\n",
      "Val Acc: 0.7679, F1: 0.7759\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0354, Acc: 0.9911\n",
      "Val Acc: 0.7946, F1: 0.7810\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0163, Acc: 0.9978\n",
      "Val Acc: 0.8036, F1: 0.7925\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0264, Acc: 0.9888\n",
      "Val Acc: 0.8036, F1: 0.7885\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0113, Acc: 0.9978\n",
      "Val Acc: 0.8214, F1: 0.8077\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0284, Acc: 0.9933\n",
      "Val Acc: 0.7768, F1: 0.7619\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0214, Acc: 0.9978\n",
      "Val Acc: 0.7946, F1: 0.7810\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0472, Acc: 0.9799\n",
      "Val Acc: 0.8125, F1: 0.7961\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0137, Acc: 0.9978\n",
      "Val Acc: 0.7946, F1: 0.7810\n",
      "Fold 4/5\n",
      "Epoch 1/50\n",
      "Train Loss: 0.5930, Acc: 0.6540\n",
      "Val Acc: 0.6071, F1: 0.0000\n",
      "Epoch 2/50\n",
      "Train Loss: 0.4315, Acc: 0.8080\n",
      "Val Acc: 0.6875, F1: 0.3636\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3649, Acc: 0.8259\n",
      "Val Acc: 0.6250, F1: 0.1250\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3262, Acc: 0.8661\n",
      "Val Acc: 0.4554, F1: 0.5850\n",
      "Epoch 5/50\n",
      "Train Loss: 0.3448, Acc: 0.8616\n",
      "Val Acc: 0.4911, F1: 0.6069\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2916, Acc: 0.8772\n",
      "Val Acc: 0.3929, F1: 0.5641\n",
      "Epoch 7/50\n",
      "Train Loss: 0.2825, Acc: 0.8795\n",
      "Val Acc: 0.6696, F1: 0.2745\n",
      "Epoch 8/50\n",
      "Train Loss: 0.2817, Acc: 0.8884\n",
      "Val Acc: 0.7857, F1: 0.6842\n",
      "Epoch 9/50\n",
      "Train Loss: 0.2349, Acc: 0.9107\n",
      "Val Acc: 0.6071, F1: 0.0000\n",
      "Epoch 10/50\n",
      "Train Loss: 0.1850, Acc: 0.9241\n",
      "Val Acc: 0.7411, F1: 0.7184\n",
      "Epoch 11/50\n",
      "Train Loss: 0.1749, Acc: 0.9263\n",
      "Val Acc: 0.6696, F1: 0.2745\n",
      "Epoch 12/50\n",
      "Train Loss: 0.2280, Acc: 0.9152\n",
      "Val Acc: 0.8304, F1: 0.7595\n",
      "Epoch 13/50\n",
      "Train Loss: 0.1677, Acc: 0.9442\n",
      "Val Acc: 0.6964, F1: 0.7069\n",
      "Epoch 14/50\n",
      "Train Loss: 0.1667, Acc: 0.9353\n",
      "Val Acc: 0.6786, F1: 0.3571\n",
      "Epoch 15/50\n",
      "Train Loss: 0.1386, Acc: 0.9509\n",
      "Val Acc: 0.8482, F1: 0.7901\n",
      "Epoch 16/50\n",
      "Train Loss: 0.1290, Acc: 0.9554\n",
      "Val Acc: 0.7232, F1: 0.4561\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1257, Acc: 0.9643\n",
      "Val Acc: 0.6964, F1: 0.4333\n",
      "Epoch 18/50\n",
      "Train Loss: 0.1215, Acc: 0.9509\n",
      "Val Acc: 0.7054, F1: 0.7227\n",
      "Epoch 19/50\n",
      "Train Loss: 0.0981, Acc: 0.9710\n",
      "Val Acc: 0.7500, F1: 0.5625\n",
      "Epoch 20/50\n",
      "Train Loss: 0.0818, Acc: 0.9665\n",
      "Val Acc: 0.8482, F1: 0.8046\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0700, Acc: 0.9732\n",
      "Val Acc: 0.8393, F1: 0.7955\n",
      "Epoch 22/50\n",
      "Train Loss: 0.1039, Acc: 0.9576\n",
      "Val Acc: 0.6875, F1: 0.3396\n",
      "Epoch 23/50\n",
      "Train Loss: 0.1070, Acc: 0.9487\n",
      "Val Acc: 0.7768, F1: 0.6479\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0722, Acc: 0.9777\n",
      "Val Acc: 0.6607, F1: 0.2692\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0828, Acc: 0.9710\n",
      "Val Acc: 0.7321, F1: 0.7170\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0638, Acc: 0.9799\n",
      "Val Acc: 0.4286, F1: 0.5789\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0441, Acc: 0.9888\n",
      "Val Acc: 0.7768, F1: 0.6988\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0593, Acc: 0.9799\n",
      "Val Acc: 0.7589, F1: 0.7429\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0457, Acc: 0.9911\n",
      "Val Acc: 0.5089, F1: 0.6099\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0310, Acc: 0.9955\n",
      "Val Acc: 0.7857, F1: 0.7647\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0366, Acc: 0.9866\n",
      "Val Acc: 0.7321, F1: 0.5312\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0656, Acc: 0.9799\n",
      "Val Acc: 0.6518, F1: 0.2642\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0423, Acc: 0.9911\n",
      "Val Acc: 0.7500, F1: 0.7407\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0492, Acc: 0.9799\n",
      "Val Acc: 0.6786, F1: 0.3333\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0845, Acc: 0.9665\n",
      "Val Acc: 0.8036, F1: 0.7381\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0328, Acc: 0.9888\n",
      "Val Acc: 0.8482, F1: 0.8090\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0355, Acc: 0.9911\n",
      "Val Acc: 0.6339, F1: 0.6667\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0240, Acc: 0.9955\n",
      "Val Acc: 0.8036, F1: 0.7755\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0435, Acc: 0.9866\n",
      "Val Acc: 0.7054, F1: 0.7179\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0285, Acc: 0.9888\n",
      "Val Acc: 0.8214, F1: 0.7826\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0332, Acc: 0.9933\n",
      "Val Acc: 0.8036, F1: 0.7381\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0441, Acc: 0.9866\n",
      "Val Acc: 0.8393, F1: 0.8085\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0369, Acc: 0.9911\n",
      "Val Acc: 0.7946, F1: 0.7579\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0258, Acc: 0.9911\n",
      "Val Acc: 0.8393, F1: 0.8043\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0240, Acc: 0.9933\n",
      "Val Acc: 0.8393, F1: 0.7907\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0175, Acc: 0.9978\n",
      "Val Acc: 0.8036, F1: 0.7609\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0192, Acc: 0.9978\n",
      "Val Acc: 0.8304, F1: 0.7912\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0612, Acc: 0.9799\n",
      "Val Acc: 0.7946, F1: 0.7416\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0193, Acc: 0.9955\n",
      "Val Acc: 0.7946, F1: 0.7473\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0362, Acc: 0.9911\n",
      "Val Acc: 0.8125, F1: 0.7692\n",
      "Fold 5/5\n",
      "Epoch 1/50\n",
      "Train Loss: 0.6426, Acc: 0.6161\n",
      "Val Acc: 0.5804, F1: 0.0000\n",
      "Epoch 2/50\n",
      "Train Loss: 0.4517, Acc: 0.7924\n",
      "Val Acc: 0.5893, F1: 0.0417\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3834, Acc: 0.8438\n",
      "Val Acc: 0.6339, F1: 0.2807\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3415, Acc: 0.8371\n",
      "Val Acc: 0.7143, F1: 0.6000\n",
      "Epoch 5/50\n",
      "Train Loss: 0.2861, Acc: 0.8862\n",
      "Val Acc: 0.5625, F1: 0.6573\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2822, Acc: 0.8750\n",
      "Val Acc: 0.6339, F1: 0.2807\n",
      "Epoch 7/50\n",
      "Train Loss: 0.2469, Acc: 0.9085\n",
      "Val Acc: 0.7679, F1: 0.7234\n",
      "Epoch 8/50\n",
      "Train Loss: 0.2373, Acc: 0.8839\n",
      "Val Acc: 0.8125, F1: 0.7921\n",
      "Epoch 9/50\n",
      "Train Loss: 0.2157, Acc: 0.9129\n",
      "Val Acc: 0.5357, F1: 0.6438\n",
      "Epoch 10/50\n",
      "Train Loss: 0.1847, Acc: 0.9375\n",
      "Val Acc: 0.6071, F1: 0.1538\n",
      "Epoch 11/50\n",
      "Train Loss: 0.1736, Acc: 0.9397\n",
      "Val Acc: 0.7679, F1: 0.6829\n",
      "Epoch 12/50\n",
      "Train Loss: 0.1727, Acc: 0.9353\n",
      "Val Acc: 0.5804, F1: 0.0000\n",
      "Epoch 13/50\n",
      "Train Loss: 0.1573, Acc: 0.9397\n",
      "Val Acc: 0.4554, F1: 0.6065\n",
      "Epoch 14/50\n",
      "Train Loss: 0.1465, Acc: 0.9397\n",
      "Val Acc: 0.7946, F1: 0.7160\n",
      "Epoch 15/50\n",
      "Train Loss: 0.1670, Acc: 0.9286\n",
      "Val Acc: 0.7857, F1: 0.7692\n",
      "Epoch 16/50\n",
      "Train Loss: 0.1245, Acc: 0.9621\n",
      "Val Acc: 0.5804, F1: 0.0000\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1730, Acc: 0.9375\n",
      "Val Acc: 0.5179, F1: 0.6351\n",
      "Epoch 18/50\n",
      "Train Loss: 0.1457, Acc: 0.9509\n",
      "Val Acc: 0.7232, F1: 0.7304\n",
      "Epoch 19/50\n",
      "Train Loss: 0.1324, Acc: 0.9487\n",
      "Val Acc: 0.5268, F1: 0.6345\n",
      "Epoch 20/50\n",
      "Train Loss: 0.1060, Acc: 0.9643\n",
      "Val Acc: 0.5804, F1: 0.0000\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0993, Acc: 0.9643\n",
      "Val Acc: 0.7946, F1: 0.7677\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0974, Acc: 0.9688\n",
      "Val Acc: 0.7768, F1: 0.6914\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0875, Acc: 0.9710\n",
      "Val Acc: 0.7232, F1: 0.5634\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0821, Acc: 0.9688\n",
      "Val Acc: 0.7768, F1: 0.6835\n",
      "Epoch 25/50\n",
      "Train Loss: 0.1484, Acc: 0.9531\n",
      "Val Acc: 0.7500, F1: 0.7358\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0801, Acc: 0.9710\n",
      "Val Acc: 0.7321, F1: 0.6250\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0524, Acc: 0.9821\n",
      "Val Acc: 0.8214, F1: 0.7561\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0711, Acc: 0.9777\n",
      "Val Acc: 0.5357, F1: 0.6438\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0469, Acc: 0.9821\n",
      "Val Acc: 0.6696, F1: 0.3729\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0601, Acc: 0.9821\n",
      "Val Acc: 0.5179, F1: 0.6301\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0493, Acc: 0.9888\n",
      "Val Acc: 0.8036, F1: 0.7250\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0580, Acc: 0.9777\n",
      "Val Acc: 0.5179, F1: 0.6351\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0405, Acc: 0.9955\n",
      "Val Acc: 0.7946, F1: 0.7229\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0311, Acc: 0.9911\n",
      "Val Acc: 0.8214, F1: 0.8000\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0406, Acc: 0.9866\n",
      "Val Acc: 0.7679, F1: 0.6486\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0630, Acc: 0.9844\n",
      "Val Acc: 0.7768, F1: 0.6753\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0324, Acc: 0.9911\n",
      "Val Acc: 0.8036, F1: 0.7500\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0293, Acc: 0.9933\n",
      "Val Acc: 0.8393, F1: 0.7955\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0343, Acc: 0.9933\n",
      "Val Acc: 0.7946, F1: 0.7579\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0375, Acc: 0.9888\n",
      "Val Acc: 0.7589, F1: 0.7216\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0363, Acc: 0.9911\n",
      "Val Acc: 0.7143, F1: 0.4839\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0281, Acc: 0.9933\n",
      "Val Acc: 0.7946, F1: 0.7089\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0179, Acc: 0.9955\n",
      "Val Acc: 0.7857, F1: 0.7273\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0375, Acc: 0.9888\n",
      "Val Acc: 0.7946, F1: 0.7229\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0176, Acc: 0.9933\n",
      "Val Acc: 0.8214, F1: 0.7872\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0290, Acc: 0.9911\n",
      "Val Acc: 0.8214, F1: 0.7826\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0176, Acc: 0.9911\n",
      "Val Acc: 0.8036, F1: 0.7609\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0323, Acc: 0.9911\n",
      "Val Acc: 0.8214, F1: 0.7778\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0312, Acc: 0.9933\n",
      "Val Acc: 0.8304, F1: 0.7957\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0282, Acc: 0.9888\n",
      "Val Acc: 0.8125, F1: 0.7742\n",
      "Average CV metrics:\n",
      "accuracy: 0.8107\n",
      "precision: 0.7866\n",
      "recall: 0.7388\n",
      "f1: 0.7592\n",
      "auc_roc: 0.9031\n",
      "New best parameters found!\n",
      "Epoch 1/50\n",
      "Train Loss: 0.6362, Acc: 0.6429\n",
      "Val Acc: 0.8227, F1: 0.7619\n",
      "Epoch 2/50\n",
      "Train Loss: 0.4612, Acc: 0.8000\n",
      "Val Acc: 0.7801, F1: 0.5974\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3874, Acc: 0.8214\n",
      "Val Acc: 0.6454, F1: 0.0000\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3596, Acc: 0.8375\n",
      "Val Acc: 0.6596, F1: 0.0769\n",
      "Epoch 5/50\n",
      "Train Loss: 0.3114, Acc: 0.8643\n",
      "Val Acc: 0.7518, F1: 0.5070\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2936, Acc: 0.8804\n",
      "Val Acc: 0.7801, F1: 0.6173\n",
      "Epoch 7/50\n",
      "Train Loss: 0.3202, Acc: 0.8679\n",
      "Val Acc: 0.6454, F1: 0.0000\n",
      "Epoch 8/50\n",
      "Train Loss: 0.2754, Acc: 0.8964\n",
      "Val Acc: 0.3830, F1: 0.5348\n",
      "Epoch 9/50\n",
      "Train Loss: 0.2221, Acc: 0.9161\n",
      "Val Acc: 0.8014, F1: 0.6500\n",
      "Epoch 10/50\n",
      "Train Loss: 0.2474, Acc: 0.8839\n",
      "Val Acc: 0.7305, F1: 0.7164\n",
      "Epoch 11/50\n",
      "Train Loss: 0.1893, Acc: 0.9179\n",
      "Val Acc: 0.7092, F1: 0.7007\n",
      "Epoch 12/50\n",
      "Train Loss: 0.2031, Acc: 0.9339\n",
      "Val Acc: 0.6454, F1: 0.0000\n",
      "Epoch 13/50\n",
      "Train Loss: 0.2320, Acc: 0.9071\n",
      "Val Acc: 0.8227, F1: 0.7706\n",
      "Epoch 14/50\n",
      "Train Loss: 0.1895, Acc: 0.9304\n",
      "Val Acc: 0.3546, F1: 0.5236\n",
      "Epoch 15/50\n",
      "Train Loss: 0.1659, Acc: 0.9393\n",
      "Val Acc: 0.7376, F1: 0.4638\n",
      "Epoch 16/50\n",
      "Train Loss: 0.1429, Acc: 0.9500\n",
      "Val Acc: 0.6667, F1: 0.1132\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1522, Acc: 0.9500\n",
      "Val Acc: 0.4255, F1: 0.5525\n",
      "Epoch 18/50\n",
      "Train Loss: 0.1547, Acc: 0.9429\n",
      "Val Acc: 0.6099, F1: 0.6358\n",
      "Epoch 19/50\n",
      "Train Loss: 0.1244, Acc: 0.9571\n",
      "Val Acc: 0.6596, F1: 0.1111\n",
      "Epoch 20/50\n",
      "Train Loss: 0.1110, Acc: 0.9571\n",
      "Val Acc: 0.7376, F1: 0.4932\n",
      "Epoch 21/50\n",
      "Train Loss: 0.1047, Acc: 0.9679\n",
      "Val Acc: 0.7163, F1: 0.3939\n",
      "Epoch 22/50\n",
      "Train Loss: 0.1069, Acc: 0.9625\n",
      "Val Acc: 0.8227, F1: 0.7253\n",
      "Epoch 23/50\n",
      "Train Loss: 0.1155, Acc: 0.9500\n",
      "Val Acc: 0.7943, F1: 0.7478\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0877, Acc: 0.9750\n",
      "Val Acc: 0.8156, F1: 0.7234\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0903, Acc: 0.9661\n",
      "Val Acc: 0.7021, F1: 0.2759\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0641, Acc: 0.9839\n",
      "Val Acc: 0.4539, F1: 0.5650\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0723, Acc: 0.9768\n",
      "Val Acc: 0.7801, F1: 0.6437\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0461, Acc: 0.9839\n",
      "Val Acc: 0.8227, F1: 0.7748\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0841, Acc: 0.9714\n",
      "Val Acc: 0.6596, F1: 0.0769\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0642, Acc: 0.9768\n",
      "Val Acc: 0.6596, F1: 0.1111\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0768, Acc: 0.9786\n",
      "Val Acc: 0.6170, F1: 0.6447\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0459, Acc: 0.9893\n",
      "Val Acc: 0.4539, F1: 0.5650\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0737, Acc: 0.9696\n",
      "Val Acc: 0.8085, F1: 0.6897\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0348, Acc: 0.9946\n",
      "Val Acc: 0.8440, F1: 0.8036\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0538, Acc: 0.9821\n",
      "Val Acc: 0.7801, F1: 0.7559\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0746, Acc: 0.9768\n",
      "Val Acc: 0.7376, F1: 0.7176\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0386, Acc: 0.9875\n",
      "Val Acc: 0.8227, F1: 0.7664\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0332, Acc: 0.9911\n",
      "Val Acc: 0.8369, F1: 0.7850\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0350, Acc: 0.9893\n",
      "Val Acc: 0.8085, F1: 0.7097\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0235, Acc: 0.9929\n",
      "Val Acc: 0.8156, F1: 0.7679\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0368, Acc: 0.9911\n",
      "Val Acc: 0.8014, F1: 0.7667\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0297, Acc: 0.9929\n",
      "Val Acc: 0.8369, F1: 0.7890\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0280, Acc: 0.9893\n",
      "Val Acc: 0.8227, F1: 0.7748\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0523, Acc: 0.9804\n",
      "Val Acc: 0.8227, F1: 0.7368\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0261, Acc: 0.9929\n",
      "Val Acc: 0.8369, F1: 0.7890\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0247, Acc: 0.9946\n",
      "Val Acc: 0.8156, F1: 0.7719\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0263, Acc: 0.9929\n",
      "Val Acc: 0.8369, F1: 0.7890\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0342, Acc: 0.9911\n",
      "Val Acc: 0.8227, F1: 0.7788\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0334, Acc: 0.9875\n",
      "Val Acc: 0.8085, F1: 0.7652\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0222, Acc: 0.9964\n",
      "Val Acc: 0.8085, F1: 0.7652\n",
      "\n",
      "Parameter combination 2/6912\n",
      "Fold 1/5\n",
      "Epoch 1/50\n",
      "Train Loss: 0.5961, Acc: 0.6853\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 2/50\n",
      "Train Loss: 0.3877, Acc: 0.8326\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3474, Acc: 0.8527\n",
      "Val Acc: 0.3571, F1: 0.5263\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3918, Acc: 0.8281\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 5/50\n",
      "Train Loss: 0.4265, Acc: 0.8259\n",
      "Val Acc: 0.7768, F1: 0.6154\n",
      "Epoch 6/50\n",
      "Train Loss: 0.4599, Acc: 0.8080\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 7/50\n",
      "Train Loss: 0.4381, Acc: 0.7902\n",
      "Val Acc: 0.4821, F1: 0.5606\n",
      "Epoch 8/50\n",
      "Train Loss: 0.4905, Acc: 0.7612\n",
      "Val Acc: 0.8036, F1: 0.6857\n",
      "Epoch 9/50\n",
      "Train Loss: 0.5013, Acc: 0.7612\n",
      "Val Acc: 0.3661, F1: 0.5359\n",
      "Epoch 10/50\n",
      "Train Loss: 2.7450, Acc: 0.5737\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 11/50\n",
      "Train Loss: 0.7054, Acc: 0.5513\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 12/50\n",
      "Train Loss: 0.6854, Acc: 0.5536\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 13/50\n",
      "Train Loss: 0.6935, Acc: 0.5692\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 14/50\n",
      "Train Loss: 0.6863, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 15/50\n",
      "Train Loss: 0.6831, Acc: 0.5804\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 16/50\n",
      "Train Loss: 0.6815, Acc: 0.5826\n",
      "Val Acc: 0.3661, F1: 0.5359\n",
      "Epoch 17/50\n",
      "Train Loss: 0.6773, Acc: 0.5826\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 18/50\n",
      "Train Loss: 0.6838, Acc: 0.5848\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 19/50\n",
      "Train Loss: 0.6796, Acc: 0.5826\n",
      "Val Acc: 0.3571, F1: 0.5263\n",
      "Epoch 20/50\n",
      "Train Loss: 0.6796, Acc: 0.5781\n",
      "Val Acc: 0.3661, F1: 0.5359\n",
      "Epoch 21/50\n",
      "Train Loss: 0.6822, Acc: 0.5692\n",
      "Val Acc: 0.3661, F1: 0.5359\n",
      "Epoch 22/50\n",
      "Train Loss: 0.6828, Acc: 0.5737\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 23/50\n",
      "Train Loss: 0.6809, Acc: 0.5826\n",
      "Val Acc: 0.3571, F1: 0.5263\n",
      "Epoch 24/50\n",
      "Train Loss: 0.6818, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 25/50\n",
      "Train Loss: 0.6823, Acc: 0.5826\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 26/50\n",
      "Train Loss: 0.6817, Acc: 0.5826\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 27/50\n",
      "Train Loss: 0.6768, Acc: 0.5826\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 28/50\n",
      "Train Loss: 0.6816, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 29/50\n",
      "Train Loss: 0.6817, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 30/50\n",
      "Train Loss: 0.6804, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 31/50\n",
      "Train Loss: 0.6802, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 32/50\n",
      "Train Loss: 0.6794, Acc: 0.5826\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Epoch 33/50\n",
      "Train Loss: 0.6827, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 34/50\n",
      "Train Loss: 0.6812, Acc: 0.5826\n",
      "Val Acc: 0.6339, F1: 0.0000\n",
      "Epoch 35/50\n",
      "Train Loss: 0.6798, Acc: 0.5826\n",
      "Val Acc: 0.6429, F1: 0.0000\n",
      "Error processing HIVP/APV.fasta: Tried to step 1001 times. The specified number of total steps is 1000\n",
      "\n",
      "All datasets processed successfully!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mdistributed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdestroy_process_group\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroy_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:1721\u001b[0m, in \u001b[0;36mdestroy_process_group\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1719\u001b[0m     pg \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _world\u001b[38;5;241m.\u001b[39mpg_map\u001b[38;5;241m.\u001b[39mget(pg, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid process group specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_fasta_data(fasta_path):\n",
    "    \"\"\"Load sequences and labels from FASTA file.\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(fasta_path, 'r') as file:\n",
    "        current_seq = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_seq:  # Save previous sequence\n",
    "                    sequences.append(''.join(current_seq))\n",
    "                    current_seq = []\n",
    "                # Extract label from header (assuming last character is label)\n",
    "                label = int(line[-1])\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                # Add sequence with spaces between amino acids\n",
    "                current_seq.append(' '.join(line))\n",
    "\n",
    "        # Add last sequence\n",
    "        if current_seq:\n",
    "            sequences.append(''.join(current_seq))\n",
    "\n",
    "    return sequences, labels\n",
    "def prepare_datasets(sequences, labels, train_ratio=0.8):\n",
    "    \"\"\"Split data into train and test sets.\"\"\"\n",
    "    # Create full dataset\n",
    "    full_dataset = ProteinDataset(sequences, labels)\n",
    "\n",
    "    # Calculate split sizes\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    test_size = total_size - train_size\n",
    "\n",
    "    # Split dataset\n",
    "    train_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "def train_single_dataset(dataset_path):\n",
    "    \"\"\"Train and evaluate a single dataset.\"\"\"\n",
    "    # Extract dataset name from path\n",
    "    dataset_name = os.path.basename(dataset_path).split('.')[0]\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "\n",
    "    # Load data\n",
    "    sequences, labels = load_fasta_data(dataset_path)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset, test_dataset = prepare_datasets(sequences, labels)\n",
    "\n",
    "    # Run grid search with cross-validation\n",
    "    best_result = run_kfold_grid_search(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        dataset_name=dataset_name,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    print(f\"Best CV accuracy: {best_result['cv_metrics']['accuracy']:.4f}\")\n",
    "    print(\"Test metrics:\", json.dumps(best_result['test_metrics'], indent=2))\n",
    "    return best_result\n",
    "def main():\n",
    "    # List of datasets to process\n",
    "    datasets = [\n",
    "        \n",
    "        \"HIVP/APV.fasta\",\n",
    "       \n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Train on each dataset sequentially\n",
    "    for dataset_path in datasets:\n",
    "        try:\n",
    "            result = train_single_dataset(dataset_path)\n",
    "            results[dataset_path] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_path}: {str(e)}\")\n",
    "        finally:\n",
    "            # Clear GPU memory after each dataset\n",
    "            clear_gpu_memory()\n",
    "\n",
    "    # Save overall results summary\n",
    "    summary_path = os.path.join('results', 'all_datasets_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(\"\\nAll datasets processed successfully!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        main()\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        clear_gpu_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            if hasattr(torch.distributed, 'destroy_process_group'):\n",
    "                torch.distributed.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b58626-a343-4526-80e7-88dcf85629bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
